{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Hybrid Model: Classification + Regression\n",
    "- Abandoned this course. Focusing on regression improvements.\n",
    "\n",
    "- Classify: popularity of 0 or not\n",
    "- Regression: what's the non-zero\n",
    " popularity score\n",
    "<br>\n",
    "\n",
    "- Classify scores to 0 (score is 0) or 1 (score is not 0)\n",
    "- For those we classified as 0, we use that prediction for the final output\n",
    "- For those > 0, we run a regression model and predict popularity score\n",
    "- Different y for each problem (\"Popularity\" vs \"class\")\n",
    "- train with the same data? Or adding an \"is 0\" feature to the training set for popularity score predictions\n",
    "- what to use classification predictions? The confirmed True Negatives? All predicted negatives?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/dataModel.csv\")\n",
    "\n",
    "## class variable is 0 or 1 for our classification framing. We can use this as our dependent variable\n",
    "df[\"class\"] = df.Popularity.apply(lambda x: 0.0 if x == 0.0 else 1.0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "outputs": [
    {
     "data": {
      "text/plain": "1.0    6574\n0.0    1926\nName: class, dtype: int64"
     },
     "execution_count": 525,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape\n",
    "df[\"class\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Binary Classification Task"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, StackingClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.dummy import DummyClassifier\n",
    "import xgboost as xgb"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "outputs": [],
   "source": [
    "def eval_classification(y_true, preds):\n",
    "    cr = classification_report(y_true, preds)\n",
    "    f1 = f1_score(y_true, preds, average='weighted')\n",
    "    cm = confusion_matrix(y_true, preds)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    # print(cm.ravel())\n",
    "    specificity = tn / (tn+fp)\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(cr)\n",
    "    print(\"\\nWeighted F1 Score:\")\n",
    "    print(f1)\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "    # print(\"Specificity: \", specificity)\n",
    "\n",
    "    return None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (8500, 17)\n"
     ]
    }
   ],
   "source": [
    "X = df.drop([\"class\", \"Popularity\"], axis=1)\n",
    "print(\"X shape:\", X.shape)\n",
    "\n",
    "imp = IterativeImputer(n_nearest_features=10)\n",
    "X = imp.fit_transform(X)\n",
    "\n",
    "y = df[\"class\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "                                                     random_state=699, stratify=y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  88  297]\n",
      " [ 304 1011]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.22      0.23      0.23       385\n",
      "         1.0       0.77      0.77      0.77      1315\n",
      "\n",
      "    accuracy                           0.65      1700\n",
      "   macro avg       0.50      0.50      0.50      1700\n",
      "weighted avg       0.65      0.65      0.65      1700\n",
      "\n",
      "0.6475913310992139\n"
     ]
    }
   ],
   "source": [
    "## DUMMY CLASSIFIER\n",
    "dummy = DummyClassifier(strategy=\"stratified\")\n",
    "dummy.fit(X_train, y_train)\n",
    "\n",
    "dummy_preds = dummy.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, dummy_preds))\n",
    "print(classification_report(y_test, dummy_preds))\n",
    "print(f1_score(y_test, dummy_preds, average=\"weighted\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "outputs": [],
   "source": [
    "## SMOTE sampling\n",
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE()\n",
    "\n",
    "## .fit_resample to oversample using SMOTE technique\n",
    "X_train_smote, y_train_smote = sm.fit_resample(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# observe that data has been balanced\n",
    "pd.Series(y_train_smote).value_counts().plot.bar(title=\"Classes are now balanced\");"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[189 196]\n",
      " [672 643]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.22      0.49      0.30       385\n",
      "         1.0       0.77      0.49      0.60      1315\n",
      "\n",
      "    accuracy                           0.49      1700\n",
      "   macro avg       0.49      0.49      0.45      1700\n",
      "weighted avg       0.64      0.49      0.53      1700\n",
      "\n",
      "0.5305238843056861\n"
     ]
    }
   ],
   "source": [
    "## DUMMY WITH SMOTE\n",
    "dummy = DummyClassifier(strategy=\"stratified\")\n",
    "dummy.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "dummy_preds = dummy.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, dummy_preds))\n",
    "print(classification_report(y_test, dummy_preds))\n",
    "print(f1_score(y_test, dummy_preds, average=\"weighted\"))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "outputs": [],
   "source": [
    "def model_performances(classifier, param_grid, sampling=True):\n",
    "    \"\"\"\n",
    "    Compare performances of different given classifiers, with and without SMOTE sampling\n",
    "    :param classifier: sklearn classifier object\n",
    "    :param param_grid: parameter grid for the appropriate classifier\n",
    "    :param sampling: Binary, whether to use SMOTE sampling or not\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', PCA()),\n",
    "        ('clf', classifier)\n",
    "    ])\n",
    "\n",
    "    ## performance without sampling\n",
    "    if not sampling:\n",
    "        print(f\"\\n{classifier} Performance without sampling...\")\n",
    "        grid = GridSearchCV(pipeline, param_grid=param_grid, scoring=\"f1\", n_jobs=-1)\n",
    "        grid.fit(X_train, y_train)\n",
    "\n",
    "        predictions = grid.predict(X_test)\n",
    "        print(f\"Best params: {grid.best_params_}\")\n",
    "        print(eval_classification(y_test, predictions))\n",
    "\n",
    "    ## performance with sampling\n",
    "    else:\n",
    "        print(f\"\\n{classifier} Performance with SMOTE sampling...\")\n",
    "        grid2 = GridSearchCV(pipeline, param_grid=param_grid, scoring=\"f1\", n_jobs=-1)\n",
    "        grid2.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "\n",
    "        predictions2 = grid2.predict(X_test)\n",
    "        print(f\"Best params: {grid2.best_params_}\")\n",
    "        print(eval_classification(y_test, predictions2))\n",
    "\n",
    "    return None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "outputs": [],
   "source": [
    "## define parameters for grid search\n",
    "\n",
    "params_LR = {'pca__n_components': np.arange(10, len(df.columns), step=2),\n",
    "             'clf__class_weight': [\"balanced\", None],\n",
    "             'clf__C': [0.01, 0.1, 1.0, 3.0],\n",
    "             'clf__penalty': ['l1', 'l2']}\n",
    "params_RF = {'pca__n_components': np.arange(10, len(df.columns), step=2),\n",
    "             'clf__class_weight': [\"balanced\", None],\n",
    "             'clf__n_estimators': [100, 300, 500, 800, 1200],\n",
    "             'clf__max_depth': [5, 8, 15, 20],\n",
    "             'clf__min_samples_split': [2, 5, 10],\n",
    "             'clf__max_features': [\"auto\", None]}\n",
    "params_KNN = {'pca__n_components':np.arange(10, len(df.columns), step=2),\n",
    "              'clf__n_neighbors': [2, 5, 10, 12],\n",
    "              'clf__weights': [\"uniform\", \"distance\"]}\n",
    "params_ADA = {'pca__n_components': np.arange(10, len(df.columns), step=2),\n",
    "              'clf__n_estimators': [50, 100, 250],\n",
    "              'clf__base_estimator': [DecisionTreeClassifier(), SVC(probability=True)]}\n",
    "params_MLP = {'pca__n_components':np.arange(10, len(df.columns), step=2),\n",
    "              'clf__activation': ['relu','tanh','logistic','identity'],\n",
    "              'clf__hidden_layer_sizes': [(100,), (150, 100, 50), (32, 16, 8)]}\n",
    "params_XGB = {'pca__n_components':np.arange(10, len(df.columns), step=2),\n",
    "              'clf__objective': ['reg:logistic','binary:hinge'],\n",
    "              'clf__min_child_weight': [1,5,10],\n",
    "              'clf__gamma': [0.5, 1.0, 1.5, 2.0],\n",
    "              'clf__subsample': [0.6, 0.8, 1.0],\n",
    "              'clf__max_depth': [3,4,5],\n",
    "              'clf__booster': [\"gbtree\", \"gblinear\", \"dart\"]}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "outputs": [],
   "source": [
    "## get list of regression, make dict with params_dict as value\n",
    "# RF takes a lot, screenshot with results capture\n",
    "try_clfs = [LogisticRegression(),\n",
    "            RandomForestClassifier(),\n",
    "            KNeighborsClassifier(),\n",
    "            AdaBoostClassifier(),\n",
    "            MLPClassifier(),\n",
    "            xgb.XGBClassifier()]\n",
    "\n",
    "s_dict = {}\n",
    "s_dict[try_clfs[0]] = params_LR\n",
    "# s_dict[try_clfs[1]] = params_RF\n",
    "s_dict[try_clfs[2]] = params_KNN\n",
    "s_dict[try_clfs[3]] = params_ADA\n",
    "s_dict[try_clfs[4]] = params_MLP\n",
    "s_dict[try_clfs[5]] = params_XGB"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LogisticRegression() Performance without sampling...\n",
      "Best params: {'clf__C': 0.01, 'clf__class_weight': None, 'clf__penalty': 'l2', 'pca__n_components': 12}\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.37      0.02      0.03       385\n",
      "         1.0       0.78      0.99      0.87      1315\n",
      "\n",
      "    accuracy                           0.77      1700\n",
      "   macro avg       0.57      0.50      0.45      1700\n",
      "weighted avg       0.68      0.77      0.68      1700\n",
      "\n",
      "\n",
      "Weighted F1 Score:\n",
      "0.6806843214754209\n",
      "\n",
      "Confusion Matrix:\n",
      "[[   7  378]\n",
      " [  12 1303]]\n",
      "None\n",
      "LogisticRegression() Performance with SMOTE sampling...\n",
      "Best params: {'clf__C': 0.01, 'clf__class_weight': 'balanced', 'clf__penalty': 'l2', 'pca__n_components': 10}\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.37      0.76      0.50       385\n",
      "         1.0       0.90      0.62      0.73      1315\n",
      "\n",
      "    accuracy                           0.65      1700\n",
      "   macro avg       0.63      0.69      0.62      1700\n",
      "weighted avg       0.78      0.65      0.68      1700\n",
      "\n",
      "\n",
      "Weighted F1 Score:\n",
      "0.6800908852749733\n",
      "\n",
      "Confusion Matrix:\n",
      "[[292  93]\n",
      " [499 816]]\n",
      "None\n",
      "\n",
      "KNeighborsClassifier() Performance without sampling...\n",
      "Best params: {'clf__n_neighbors': 12, 'clf__weights': 'distance', 'pca__n_components': 16}\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.47      0.15      0.23       385\n",
      "         1.0       0.79      0.95      0.86      1315\n",
      "\n",
      "    accuracy                           0.77      1700\n",
      "   macro avg       0.63      0.55      0.54      1700\n",
      "weighted avg       0.72      0.77      0.72      1700\n",
      "\n",
      "\n",
      "Weighted F1 Score:\n",
      "0.7197758127318714\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  57  328]\n",
      " [  64 1251]]\n",
      "None\n",
      "KNeighborsClassifier() Performance with SMOTE sampling...\n",
      "Best params: {'clf__n_neighbors': 2, 'clf__weights': 'distance', 'pca__n_components': 16}\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.30      0.42      0.35       385\n",
      "         1.0       0.81      0.71      0.76      1315\n",
      "\n",
      "    accuracy                           0.65      1700\n",
      "   macro avg       0.55      0.56      0.55      1700\n",
      "weighted avg       0.69      0.65      0.66      1700\n",
      "\n",
      "\n",
      "Weighted F1 Score:\n",
      "0.6642113428333942\n",
      "\n",
      "Confusion Matrix:\n",
      "[[160 225]\n",
      " [377 938]]\n",
      "None\n",
      "\n",
      "AdaBoostClassifier() Performance without sampling...\n",
      "Best params: {'clf__base_estimator': SVC(probability=True), 'clf__n_estimators': 50, 'pca__n_components': 10}\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       385\n",
      "         1.0       0.77      1.00      0.87      1315\n",
      "\n",
      "    accuracy                           0.77      1700\n",
      "   macro avg       0.39      0.50      0.44      1700\n",
      "weighted avg       0.60      0.77      0.67      1700\n",
      "\n",
      "\n",
      "Weighted F1 Score:\n",
      "0.6747536825675544\n",
      "\n",
      "Confusion Matrix:\n",
      "[[   0  385]\n",
      " [   0 1315]]\n",
      "None\n",
      "AdaBoostClassifier() Performance with SMOTE sampling...\n",
      "Best params: {'clf__base_estimator': SVC(probability=True), 'clf__n_estimators': 250, 'pca__n_components': 16}\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.26      0.68      0.38       385\n",
      "         1.0       0.82      0.44      0.57      1315\n",
      "\n",
      "    accuracy                           0.49      1700\n",
      "   macro avg       0.54      0.56      0.47      1700\n",
      "weighted avg       0.69      0.49      0.53      1700\n",
      "\n",
      "\n",
      "Weighted F1 Score:\n",
      "0.5258867664907737\n",
      "\n",
      "Confusion Matrix:\n",
      "[[260 125]\n",
      " [741 574]]\n",
      "None\n",
      "\n",
      "MLPClassifier() Performance without sampling...\n",
      "Best params: {'clf__activation': 'logistic', 'clf__hidden_layer_sizes': (150, 100, 50), 'pca__n_components': 12}\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       385\n",
      "         1.0       0.77      1.00      0.87      1315\n",
      "\n",
      "    accuracy                           0.77      1700\n",
      "   macro avg       0.39      0.50      0.44      1700\n",
      "weighted avg       0.60      0.77      0.67      1700\n",
      "\n",
      "\n",
      "Weighted F1 Score:\n",
      "0.6747536825675544\n",
      "\n",
      "Confusion Matrix:\n",
      "[[   0  385]\n",
      " [   0 1315]]\n",
      "None\n",
      "MLPClassifier() Performance with SMOTE sampling...\n",
      "Best params: {'clf__activation': 'relu', 'clf__hidden_layer_sizes': (150, 100, 50), 'pca__n_components': 16}\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.34      0.35      0.34       385\n",
      "         1.0       0.81      0.80      0.81      1315\n",
      "\n",
      "    accuracy                           0.70      1700\n",
      "   macro avg       0.57      0.57      0.57      1700\n",
      "weighted avg       0.70      0.70      0.70      1700\n",
      "\n",
      "\n",
      "Weighted F1 Score:\n",
      "0.7008168060095549\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 133  252]\n",
      " [ 258 1057]]\n",
      "None\n",
      "\n",
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None, gamma=None,\n",
      "              gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
      "              learning_rate=None, max_delta_step=None, max_depth=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              random_state=None, reg_alpha=None, reg_lambda=None,\n",
      "              scale_pos_weight=None, subsample=None, tree_method=None,\n",
      "              validate_parameters=None, verbosity=None) Performance without sampling...\n",
      "Best params: {'clf__booster': 'dart', 'clf__gamma': 2.0, 'clf__max_depth': 3, 'clf__min_child_weight': 10, 'clf__objective': 'binary:hinge', 'clf__subsample': 0.8, 'pca__n_components': 14}\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.08      0.15       385\n",
      "         1.0       0.79      0.98      0.87      1315\n",
      "\n",
      "    accuracy                           0.78      1700\n",
      "   macro avg       0.69      0.53      0.51      1700\n",
      "weighted avg       0.74      0.78      0.71      1700\n",
      "\n",
      "\n",
      "Weighted F1 Score:\n",
      "0.708580907281732\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  32  353]\n",
      " [  22 1293]]\n",
      "None\n",
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None, gamma=None,\n",
      "              gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
      "              learning_rate=None, max_delta_step=None, max_depth=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              random_state=None, reg_alpha=None, reg_lambda=None,\n",
      "              scale_pos_weight=None, subsample=None, tree_method=None,\n",
      "              validate_parameters=None, verbosity=None) Performance with SMOTE sampling...\n",
      "Best params: {'clf__booster': 'dart', 'clf__gamma': 1.5, 'clf__max_depth': 5, 'clf__min_child_weight': 1, 'clf__objective': 'reg:logistic', 'clf__subsample': 0.8, 'pca__n_components': 16}\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.36      0.44      0.40       385\n",
      "         1.0       0.82      0.77      0.79      1315\n",
      "\n",
      "    accuracy                           0.69      1700\n",
      "   macro avg       0.59      0.60      0.59      1700\n",
      "weighted avg       0.72      0.69      0.70      1700\n",
      "\n",
      "\n",
      "Weighted F1 Score:\n",
      "0.7034822649516762\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 171  214]\n",
      " [ 309 1006]]\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/MSD/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan 0.71370371\n",
      " 0.70915934 0.70453039 0.70482119        nan        nan        nan\n",
      "        nan        nan        nan 0.87044432 0.87123491 0.87103358\n",
      " 0.87023361        nan        nan        nan        nan        nan\n",
      "        nan 0.71273671 0.70841966 0.70415582 0.70497527        nan\n",
      "        nan        nan        nan        nan        nan 0.86972297\n",
      " 0.87028486 0.86804762 0.86906872        nan        nan        nan\n",
      "        nan        nan        nan 0.71244278 0.70856792 0.70430708\n",
      " 0.70504117        nan        nan        nan        nan        nan\n",
      "        nan 0.86984674 0.87007355 0.86805321 0.86903076        nan\n",
      "        nan        nan        nan        nan        nan 0.71244278\n",
      " 0.70856792 0.70430708 0.70504117        nan        nan        nan\n",
      "        nan        nan        nan 0.86984674 0.87007355 0.86795722\n",
      " 0.86903076        nan]\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/MSD/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan 0.66335796\n",
      " 0.6593293  0.65365971 0.65488914        nan        nan        nan\n",
      "        nan        nan        nan 0.66335796 0.6593293  0.6535137\n",
      " 0.65488914        nan        nan        nan        nan        nan\n",
      "        nan 0.66321272 0.65863056 0.65287888 0.6546783         nan\n",
      "        nan        nan        nan        nan        nan 0.66321272\n",
      " 0.65863056 0.65287888 0.6546783         nan        nan        nan\n",
      "        nan        nan        nan 0.66314067 0.65862517 0.65294895\n",
      " 0.65468032        nan        nan        nan        nan        nan\n",
      "        nan 0.66314067 0.65862517 0.65294895 0.65468032        nan\n",
      "        nan        nan        nan        nan        nan 0.66300018\n",
      " 0.65862517 0.65294895 0.65468032        nan        nan        nan\n",
      "        nan        nan        nan 0.66300018 0.65862517 0.65294895\n",
      " 0.65468032        nan]\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/MSD/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.73119343 0.73529283 0.73815045 0.74136245        nan 0.80357499\n",
      " 0.80458054 0.80465483 0.80300744        nan 0.84649088 0.84885595\n",
      " 0.84868582 0.84708891        nan 0.84643691 0.84867065 0.84915894\n",
      " 0.8474873         nan 0.84982173 0.85098781 0.85237019 0.85317672\n",
      "        nan 0.85712359 0.85868338 0.86125927 0.86058762        nan\n",
      " 0.85692167 0.85621935 0.85865527 0.85723379        nan 0.86194413\n",
      " 0.86245982 0.86273627 0.86276522        nan]\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/MSD/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.63196731 0.66893074 0.67960556 0.67695778        nan 0.74915118\n",
      " 0.78298229 0.79265311 0.79987695        nan 0.70154396 0.72435907\n",
      " 0.7291184  0.73326414        nan 0.72174677 0.74682757 0.75164401\n",
      " 0.75071685        nan 0.66305499 0.67722727 0.68116194 0.67905657\n",
      "        nan 0.72394459 0.74793881 0.75039565 0.74799967        nan\n",
      " 0.65901384 0.67872016 0.67674788 0.67171108        nan 0.71978424\n",
      " 0.73920438 0.74131232 0.73614104        nan]\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/MSD/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.79079197 0.79433013 0.79263047 0.79604382        nan 0.78814167\n",
      " 0.79403489 0.79281481 0.79711505        nan 0.79192831 0.79752007\n",
      " 0.79424525 0.7976382         nan 0.8722116  0.8722116  0.8722116\n",
      " 0.8722116         nan 0.8722116  0.8722116  0.8722116  0.8722116\n",
      "        nan 0.8722116  0.8722116  0.8722116  0.8722116         nan]\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/MSD/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/envs/MSD/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/envs/MSD/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/envs/MSD/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.67596318 0.69228268 0.69795413 0.69489734        nan 0.6733279\n",
      " 0.69029111 0.69932158 0.6969272         nan 0.67410242 0.69138638\n",
      " 0.6981255  0.69381309        nan 0.51377631 0.6686994  0.67322968\n",
      " 0.70031454        nan 0.67022199 0.66674848 0.68763902 0.69439344\n",
      "        nan 0.67295542 0.71686387 0.68201455 0.75877357        nan]\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/MSD/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.86331674 0.86366079 0.8607152  0.86126796        nan 0.81474864\n",
      " 0.81262843 0.81657714 0.82344049        nan 0.84584646 0.84646871\n",
      " 0.84705261 0.8381893         nan 0.86257434 0.86148549 0.85754055\n",
      " 0.85158128        nan 0.8217721  0.81858172 0.81821631 0.8165368\n",
      "        nan 0.84926838 0.84800213 0.84140705 0.8400408         nan\n",
      " 0.86996464 0.87055943 0.86944552 0.86904716        nan 0.87165326\n",
      " 0.87254929 0.8684261  0.87005406        nan 0.8722116  0.8722116\n",
      " 0.8722116  0.8721599         nan 0.86945527 0.86976791 0.86814696\n",
      " 0.86906255        nan 0.86799205 0.87090644 0.86678633 0.86864058\n",
      "        nan 0.87025197 0.86743165 0.86786092 0.86783148        nan]\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/MSD/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/envs/MSD/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/envs/MSD/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/envs/MSD/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.71878013 0.76168833 0.77645869 0.78908313        nan 0.77935761\n",
      " 0.79474285 0.80028156 0.82056887        nan 0.71241484 0.74712463\n",
      " 0.76048205 0.76384626        nan 0.71502685 0.7581999  0.77417894\n",
      " 0.77834412        nan 0.76901002 0.796969   0.80313109 0.813847\n",
      "        nan 0.71398128 0.75462578 0.76801268 0.76654046        nan\n",
      " 0.68338717 0.71366124 0.72371316 0.73206201        nan 0.65869676\n",
      " 0.68021487 0.68925294 0.69400985        nan 0.68882728 0.70752625\n",
      " 0.72576135 0.75199235        nan 0.66307677 0.65853435 0.65351848\n",
      " 0.65466954        nan 0.66430854 0.6594952  0.65289747 0.65531058\n",
      "        nan 0.66277155 0.65876619 0.65062953 0.65480766        nan]\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/MSD/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.85621219 0.85608441 0.8587198  ... 0.85603072 0.85408112        nan]\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/MSD/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/opt/anaconda3/envs/MSD/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.70419968 0.72356575 0.73419804 ... 0.75075116 0.75707756        nan]\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/MSD/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "## run the gridsearch for every model\n",
    "for clf, params in s_dict.items():\n",
    "    # print(v)\n",
    "    model_performances(clf, params)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Findings:\n",
    "- SMOTE helps in our model performance\n",
    "- Best performing models: Random Forest (best), MLP (), XGBoost (Stacking Classifier?)\n",
    "- Best performing PCA components all hover around 14-16\n",
    "-"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "outputs": [],
   "source": [
    "### pick estimators to stack based on performance on previous experiment (above)\n",
    "\n",
    "estimators = [\n",
    "        ('est1', RandomForestClassifier(class_weight=\"balanced\", max_depth=20,\n",
    "                                        max_features=\"auto\", min_samples_split=5,\n",
    "                                        random_state=699)),\n",
    "        ('est2', MLPClassifier(activation=\"relu\", hidden_layer_sizes=(150,100,50),\n",
    "                                random_state=699)),\n",
    "        ('est3', xgb.XGBClassifier(booster=\"dart\", gamma=1.5, max_depth=5,\n",
    "                                   min_child_weight=1, objective=\"reg:logistic\",\n",
    "                                   subsample=0.8)),\n",
    "        ('est4', LinearSVC(class_weight=\"balanced\", random_state=699))]\n",
    "\n",
    "stack_clf = StackingClassifier(\n",
    "    estimators=estimators, final_estimator=LogisticRegression(class_weight=\"balanced\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/MSD/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/opt/anaconda3/envs/MSD/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/envs/MSD/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/opt/anaconda3/envs/MSD/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/opt/anaconda3/envs/MSD/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/opt/anaconda3/envs/MSD/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/opt/anaconda3/envs/MSD/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/opt/anaconda3/envs/MSD/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/envs/MSD/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/envs/MSD/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/envs/MSD/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/envs/MSD/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.43      0.39      0.41       385\n",
      "         1.0       0.83      0.85      0.84      1315\n",
      "\n",
      "    accuracy                           0.74      1700\n",
      "   macro avg       0.63      0.62      0.62      1700\n",
      "weighted avg       0.74      0.74      0.74      1700\n",
      "\n",
      "\n",
      "Weighted F1 Score:\n",
      "0.7390468865179548\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 151  234]\n",
      " [ 203 1112]]\n"
     ]
    }
   ],
   "source": [
    "# Best params: {'clf__booster': 'dart', 'clf__gamma': 1.5, 'clf__max_depth': 5, 'clf__min_child_weight': 1, 'clf__objective': 'reg:logistic', 'clf__subsample': 0.8, 'pca__n_components': 16}\n",
    "# Best params: {'clf__activation': 'relu', 'clf__hidden_layer_sizes': (150, 100, 50), 'pca__n_components': 16}\n",
    "\n",
    "# clf.fit(X_train, y_train)\n",
    "stack_clf.fit(X_train_smote, y_train_smote)\n",
    "stack_clf_preds = stack_clf.predict(X_test)\n",
    "\n",
    "eval_classification(y_test, stack_clf_preds)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "StackingClassifier(estimators=[('est1',\n",
      "                                RandomForestClassifier(class_weight='balanced',\n",
      "                                                       max_depth=20,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       random_state=699)),\n",
      "                               ('est2',\n",
      "                                MLPClassifier(hidden_layer_sizes=(150, 100, 2),\n",
      "                                              learning_rate='adaptive',\n",
      "                                              random_state=699)),\n",
      "                               ('est3',\n",
      "                                XGBClassifier(base_score=None, booster='dart',\n",
      "                                              colsample_bylevel=None,\n",
      "                                              colsample_bynode=None,\n",
      "                                              colsample_bytree=...\n",
      "                                              monotone_constraints=None,\n",
      "                                              n_estimators=100, n_jobs=None,\n",
      "                                              num_parallel_tree=None,\n",
      "                                              objective='reg:logistic',\n",
      "                                              random_state=None, reg_alpha=None,\n",
      "                                              reg_lambda=None,\n",
      "                                              scale_pos_weight=None,\n",
      "                                              subsample=0.8, tree_method=None,\n",
      "                                              validate_parameters=None,\n",
      "                                              verbosity=None)),\n",
      "                               ('est4',\n",
      "                                LinearSVC(class_weight='balanced',\n",
      "                                          random_state=699))],\n",
      "                   final_estimator=LogisticRegression(class_weight='balanced')) Performance with SMOTE sampling...\n",
      "Best params: {'pca__n_components': 16}\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.42      0.35      0.38       385\n",
      "         1.0       0.82      0.86      0.84      1315\n",
      "\n",
      "    accuracy                           0.74      1700\n",
      "   macro avg       0.62      0.61      0.61      1700\n",
      "weighted avg       0.73      0.74      0.74      1700\n",
      "\n",
      "\n",
      "Weighted F1 Score:\n",
      "0.7358638614941525\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 135  250]\n",
      " [ 184 1131]]\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/MSD/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/opt/anaconda3/envs/MSD/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/envs/MSD/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/opt/anaconda3/envs/MSD/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/opt/anaconda3/envs/MSD/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/opt/anaconda3/envs/MSD/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/opt/anaconda3/envs/MSD/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/opt/anaconda3/envs/MSD/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/envs/MSD/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/envs/MSD/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/envs/MSD/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/envs/MSD/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    }
   ],
   "source": [
    "# params_STACK = {'pca__n_components': [16]}\n",
    "\n",
    "# eval_classification(y_test, stack_clf_preds)\n",
    "# model_performances(stack_clf, param_grid=params_STACK)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/MSD/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/opt/anaconda3/envs/MSD/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/envs/MSD/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/opt/anaconda3/envs/MSD/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/opt/anaconda3/envs/MSD/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/opt/anaconda3/envs/MSD/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/opt/anaconda3/envs/MSD/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/opt/anaconda3/envs/MSD/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/envs/MSD/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/envs/MSD/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/envs/MSD/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/opt/anaconda3/envs/MSD/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    }
   ],
   "source": [
    "print(len(stack_clf_preds))\n",
    "\n",
    "stack_clf.fit(X, y)\n",
    "full_preds = stack_clf.predict(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "outputs": [
    {
     "data": {
      "text/plain": "      ArtistName  KeySignatureConfidence  TimeSignatureConfidence  Year  \\\n0           2678                   0.591                    0.372  2008   \n1           1735                   0.429                    0.533  2004   \n2           3176                   0.000                    0.000     0   \n3           1441                   0.380                    0.369  2007   \n4           2456                   0.551                    1.000     0   \n...          ...                     ...                      ...   ...   \n8495         464                   0.444                    0.243     0   \n8496        3510                   0.756                    0.536  1981   \n8497        3681                   0.180                    0.307  2007   \n8498         996                   0.042                    0.484  2007   \n8499        3556                   0.608                    0.345     0   \n\n      Popularity  danceability  energy   key  loudness  mode  speechiness  \\\n0           19.0        0.6560  0.4870   0.0    -9.344   1.0       0.0360   \n1            3.0        0.3410  0.3160   1.0    -8.922   0.0       0.0335   \n2            4.0        0.5140  0.2030   7.0   -15.443   1.0       0.0350   \n3            1.0        0.1920  0.9870   1.0    -5.382   1.0       0.1110   \n4            0.0        0.3340  0.2000  10.0   -15.672   1.0       0.0371   \n...          ...           ...     ...   ...       ...   ...          ...   \n8495         0.0        0.6910  0.8570   7.0    -5.879   1.0       0.0315   \n8496        44.0        0.4710  0.0365   9.0   -18.460   0.0       0.0608   \n8497        23.0        0.5540  0.9260   2.0    -6.306   1.0       0.0775   \n8498        39.0        0.6810  0.5920   8.0   -12.565   1.0       0.0717   \n8499         0.0        0.0886  0.1580  10.0   -15.528   0.0       0.0368   \n\n      acousticness  instrumentalness  liveness  valence    tempo  duration_ms  \\\n0         0.077800          0.056300     0.108   0.0385  123.974     148701.0   \n1         0.886000          0.000012     0.104   0.1680   80.283     253000.0   \n2         0.906000          0.830000     0.139   0.5400   83.179     240400.0   \n3         0.000011          0.681000     0.818   0.2020  103.886     138760.0   \n4         0.925000          0.694000     0.112   0.3200  119.185     199784.0   \n...            ...               ...       ...      ...      ...          ...   \n8495      0.155000          0.000006     0.212   0.9050  143.004     237160.0   \n8496      0.959000          0.808000     0.099   0.1180  172.646     141040.0   \n8497      0.000040          0.000070     0.338   0.3790  109.994     203430.0   \n8498      0.089900          0.018000     0.103   0.6720   79.943     238600.0   \n8499      0.720000          0.964000     0.115   0.0376   86.050     117840.0   \n\n      time_signature  class  score_prediction  \n0                4.0    1.0               1.0  \n1                4.0    1.0               1.0  \n2                4.0    1.0               1.0  \n3                4.0    1.0               1.0  \n4                4.0    0.0               0.0  \n...              ...    ...               ...  \n8495             4.0    0.0               0.0  \n8496             4.0    1.0               1.0  \n8497             4.0    1.0               1.0  \n8498             4.0    1.0               1.0  \n8499             4.0    0.0               0.0  \n\n[8500 rows x 20 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ArtistName</th>\n      <th>KeySignatureConfidence</th>\n      <th>TimeSignatureConfidence</th>\n      <th>Year</th>\n      <th>Popularity</th>\n      <th>danceability</th>\n      <th>energy</th>\n      <th>key</th>\n      <th>loudness</th>\n      <th>mode</th>\n      <th>speechiness</th>\n      <th>acousticness</th>\n      <th>instrumentalness</th>\n      <th>liveness</th>\n      <th>valence</th>\n      <th>tempo</th>\n      <th>duration_ms</th>\n      <th>time_signature</th>\n      <th>class</th>\n      <th>score_prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2678</td>\n      <td>0.591</td>\n      <td>0.372</td>\n      <td>2008</td>\n      <td>19.0</td>\n      <td>0.6560</td>\n      <td>0.4870</td>\n      <td>0.0</td>\n      <td>-9.344</td>\n      <td>1.0</td>\n      <td>0.0360</td>\n      <td>0.077800</td>\n      <td>0.056300</td>\n      <td>0.108</td>\n      <td>0.0385</td>\n      <td>123.974</td>\n      <td>148701.0</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1735</td>\n      <td>0.429</td>\n      <td>0.533</td>\n      <td>2004</td>\n      <td>3.0</td>\n      <td>0.3410</td>\n      <td>0.3160</td>\n      <td>1.0</td>\n      <td>-8.922</td>\n      <td>0.0</td>\n      <td>0.0335</td>\n      <td>0.886000</td>\n      <td>0.000012</td>\n      <td>0.104</td>\n      <td>0.1680</td>\n      <td>80.283</td>\n      <td>253000.0</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3176</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0</td>\n      <td>4.0</td>\n      <td>0.5140</td>\n      <td>0.2030</td>\n      <td>7.0</td>\n      <td>-15.443</td>\n      <td>1.0</td>\n      <td>0.0350</td>\n      <td>0.906000</td>\n      <td>0.830000</td>\n      <td>0.139</td>\n      <td>0.5400</td>\n      <td>83.179</td>\n      <td>240400.0</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1441</td>\n      <td>0.380</td>\n      <td>0.369</td>\n      <td>2007</td>\n      <td>1.0</td>\n      <td>0.1920</td>\n      <td>0.9870</td>\n      <td>1.0</td>\n      <td>-5.382</td>\n      <td>1.0</td>\n      <td>0.1110</td>\n      <td>0.000011</td>\n      <td>0.681000</td>\n      <td>0.818</td>\n      <td>0.2020</td>\n      <td>103.886</td>\n      <td>138760.0</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2456</td>\n      <td>0.551</td>\n      <td>1.000</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.3340</td>\n      <td>0.2000</td>\n      <td>10.0</td>\n      <td>-15.672</td>\n      <td>1.0</td>\n      <td>0.0371</td>\n      <td>0.925000</td>\n      <td>0.694000</td>\n      <td>0.112</td>\n      <td>0.3200</td>\n      <td>119.185</td>\n      <td>199784.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>8495</th>\n      <td>464</td>\n      <td>0.444</td>\n      <td>0.243</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.6910</td>\n      <td>0.8570</td>\n      <td>7.0</td>\n      <td>-5.879</td>\n      <td>1.0</td>\n      <td>0.0315</td>\n      <td>0.155000</td>\n      <td>0.000006</td>\n      <td>0.212</td>\n      <td>0.9050</td>\n      <td>143.004</td>\n      <td>237160.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>8496</th>\n      <td>3510</td>\n      <td>0.756</td>\n      <td>0.536</td>\n      <td>1981</td>\n      <td>44.0</td>\n      <td>0.4710</td>\n      <td>0.0365</td>\n      <td>9.0</td>\n      <td>-18.460</td>\n      <td>0.0</td>\n      <td>0.0608</td>\n      <td>0.959000</td>\n      <td>0.808000</td>\n      <td>0.099</td>\n      <td>0.1180</td>\n      <td>172.646</td>\n      <td>141040.0</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>8497</th>\n      <td>3681</td>\n      <td>0.180</td>\n      <td>0.307</td>\n      <td>2007</td>\n      <td>23.0</td>\n      <td>0.5540</td>\n      <td>0.9260</td>\n      <td>2.0</td>\n      <td>-6.306</td>\n      <td>1.0</td>\n      <td>0.0775</td>\n      <td>0.000040</td>\n      <td>0.000070</td>\n      <td>0.338</td>\n      <td>0.3790</td>\n      <td>109.994</td>\n      <td>203430.0</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>8498</th>\n      <td>996</td>\n      <td>0.042</td>\n      <td>0.484</td>\n      <td>2007</td>\n      <td>39.0</td>\n      <td>0.6810</td>\n      <td>0.5920</td>\n      <td>8.0</td>\n      <td>-12.565</td>\n      <td>1.0</td>\n      <td>0.0717</td>\n      <td>0.089900</td>\n      <td>0.018000</td>\n      <td>0.103</td>\n      <td>0.6720</td>\n      <td>79.943</td>\n      <td>238600.0</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>8499</th>\n      <td>3556</td>\n      <td>0.608</td>\n      <td>0.345</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0886</td>\n      <td>0.1580</td>\n      <td>10.0</td>\n      <td>-15.528</td>\n      <td>0.0</td>\n      <td>0.0368</td>\n      <td>0.720000</td>\n      <td>0.964000</td>\n      <td>0.115</td>\n      <td>0.0376</td>\n      <td>86.050</td>\n      <td>117840.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>8500 rows  20 columns</p>\n</div>"
     },
     "execution_count": 602,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"score_prediction\"] = full_preds\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "outputs": [
    {
     "data": {
      "text/plain": "      ArtistName  KeySignatureConfidence  TimeSignatureConfidence  Year  \\\n0           2678                   0.591                    0.372  2008   \n1           1735                   0.429                    0.533  2004   \n2           3176                   0.000                    0.000     0   \n3           1441                   0.380                    0.369  2007   \n4           2456                   0.551                    1.000     0   \n...          ...                     ...                      ...   ...   \n8495         464                   0.444                    0.243     0   \n8496        3510                   0.756                    0.536  1981   \n8497        3681                   0.180                    0.307  2007   \n8498         996                   0.042                    0.484  2007   \n8499        3556                   0.608                    0.345     0   \n\n      Popularity  danceability  energy   key  loudness  mode  speechiness  \\\n0           19.0        0.6560  0.4870   0.0    -9.344   1.0       0.0360   \n1            3.0        0.3410  0.3160   1.0    -8.922   0.0       0.0335   \n2            4.0        0.5140  0.2030   7.0   -15.443   1.0       0.0350   \n3            1.0        0.1920  0.9870   1.0    -5.382   1.0       0.1110   \n4            0.0        0.3340  0.2000  10.0   -15.672   1.0       0.0371   \n...          ...           ...     ...   ...       ...   ...          ...   \n8495         0.0        0.6910  0.8570   7.0    -5.879   1.0       0.0315   \n8496        44.0        0.4710  0.0365   9.0   -18.460   0.0       0.0608   \n8497        23.0        0.5540  0.9260   2.0    -6.306   1.0       0.0775   \n8498        39.0        0.6810  0.5920   8.0   -12.565   1.0       0.0717   \n8499         0.0        0.0886  0.1580  10.0   -15.528   0.0       0.0368   \n\n      acousticness  instrumentalness  liveness  valence    tempo  duration_ms  \\\n0         0.077800          0.056300     0.108   0.0385  123.974     148701.0   \n1         0.886000          0.000012     0.104   0.1680   80.283     253000.0   \n2         0.906000          0.830000     0.139   0.5400   83.179     240400.0   \n3         0.000011          0.681000     0.818   0.2020  103.886     138760.0   \n4         0.925000          0.694000     0.112   0.3200  119.185     199784.0   \n...            ...               ...       ...      ...      ...          ...   \n8495      0.155000          0.000006     0.212   0.9050  143.004     237160.0   \n8496      0.959000          0.808000     0.099   0.1180  172.646     141040.0   \n8497      0.000040          0.000070     0.338   0.3790  109.994     203430.0   \n8498      0.089900          0.018000     0.103   0.6720   79.943     238600.0   \n8499      0.720000          0.964000     0.115   0.0376   86.050     117840.0   \n\n      time_signature  class  score_prediction  \n0                4.0    1.0               1.0  \n1                4.0    1.0               1.0  \n2                4.0    1.0               1.0  \n3                4.0    1.0               1.0  \n4                4.0    0.0               0.0  \n...              ...    ...               ...  \n8495             4.0    0.0               0.0  \n8496             4.0    1.0               1.0  \n8497             4.0    1.0               1.0  \n8498             4.0    1.0               1.0  \n8499             4.0    0.0               0.0  \n\n[8500 rows x 20 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ArtistName</th>\n      <th>KeySignatureConfidence</th>\n      <th>TimeSignatureConfidence</th>\n      <th>Year</th>\n      <th>Popularity</th>\n      <th>danceability</th>\n      <th>energy</th>\n      <th>key</th>\n      <th>loudness</th>\n      <th>mode</th>\n      <th>speechiness</th>\n      <th>acousticness</th>\n      <th>instrumentalness</th>\n      <th>liveness</th>\n      <th>valence</th>\n      <th>tempo</th>\n      <th>duration_ms</th>\n      <th>time_signature</th>\n      <th>class</th>\n      <th>score_prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2678</td>\n      <td>0.591</td>\n      <td>0.372</td>\n      <td>2008</td>\n      <td>19.0</td>\n      <td>0.6560</td>\n      <td>0.4870</td>\n      <td>0.0</td>\n      <td>-9.344</td>\n      <td>1.0</td>\n      <td>0.0360</td>\n      <td>0.077800</td>\n      <td>0.056300</td>\n      <td>0.108</td>\n      <td>0.0385</td>\n      <td>123.974</td>\n      <td>148701.0</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1735</td>\n      <td>0.429</td>\n      <td>0.533</td>\n      <td>2004</td>\n      <td>3.0</td>\n      <td>0.3410</td>\n      <td>0.3160</td>\n      <td>1.0</td>\n      <td>-8.922</td>\n      <td>0.0</td>\n      <td>0.0335</td>\n      <td>0.886000</td>\n      <td>0.000012</td>\n      <td>0.104</td>\n      <td>0.1680</td>\n      <td>80.283</td>\n      <td>253000.0</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3176</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>0</td>\n      <td>4.0</td>\n      <td>0.5140</td>\n      <td>0.2030</td>\n      <td>7.0</td>\n      <td>-15.443</td>\n      <td>1.0</td>\n      <td>0.0350</td>\n      <td>0.906000</td>\n      <td>0.830000</td>\n      <td>0.139</td>\n      <td>0.5400</td>\n      <td>83.179</td>\n      <td>240400.0</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1441</td>\n      <td>0.380</td>\n      <td>0.369</td>\n      <td>2007</td>\n      <td>1.0</td>\n      <td>0.1920</td>\n      <td>0.9870</td>\n      <td>1.0</td>\n      <td>-5.382</td>\n      <td>1.0</td>\n      <td>0.1110</td>\n      <td>0.000011</td>\n      <td>0.681000</td>\n      <td>0.818</td>\n      <td>0.2020</td>\n      <td>103.886</td>\n      <td>138760.0</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2456</td>\n      <td>0.551</td>\n      <td>1.000</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.3340</td>\n      <td>0.2000</td>\n      <td>10.0</td>\n      <td>-15.672</td>\n      <td>1.0</td>\n      <td>0.0371</td>\n      <td>0.925000</td>\n      <td>0.694000</td>\n      <td>0.112</td>\n      <td>0.3200</td>\n      <td>119.185</td>\n      <td>199784.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>8495</th>\n      <td>464</td>\n      <td>0.444</td>\n      <td>0.243</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.6910</td>\n      <td>0.8570</td>\n      <td>7.0</td>\n      <td>-5.879</td>\n      <td>1.0</td>\n      <td>0.0315</td>\n      <td>0.155000</td>\n      <td>0.000006</td>\n      <td>0.212</td>\n      <td>0.9050</td>\n      <td>143.004</td>\n      <td>237160.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>8496</th>\n      <td>3510</td>\n      <td>0.756</td>\n      <td>0.536</td>\n      <td>1981</td>\n      <td>44.0</td>\n      <td>0.4710</td>\n      <td>0.0365</td>\n      <td>9.0</td>\n      <td>-18.460</td>\n      <td>0.0</td>\n      <td>0.0608</td>\n      <td>0.959000</td>\n      <td>0.808000</td>\n      <td>0.099</td>\n      <td>0.1180</td>\n      <td>172.646</td>\n      <td>141040.0</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>8497</th>\n      <td>3681</td>\n      <td>0.180</td>\n      <td>0.307</td>\n      <td>2007</td>\n      <td>23.0</td>\n      <td>0.5540</td>\n      <td>0.9260</td>\n      <td>2.0</td>\n      <td>-6.306</td>\n      <td>1.0</td>\n      <td>0.0775</td>\n      <td>0.000040</td>\n      <td>0.000070</td>\n      <td>0.338</td>\n      <td>0.3790</td>\n      <td>109.994</td>\n      <td>203430.0</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>8498</th>\n      <td>996</td>\n      <td>0.042</td>\n      <td>0.484</td>\n      <td>2007</td>\n      <td>39.0</td>\n      <td>0.6810</td>\n      <td>0.5920</td>\n      <td>8.0</td>\n      <td>-12.565</td>\n      <td>1.0</td>\n      <td>0.0717</td>\n      <td>0.089900</td>\n      <td>0.018000</td>\n      <td>0.103</td>\n      <td>0.6720</td>\n      <td>79.943</td>\n      <td>238600.0</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>8499</th>\n      <td>3556</td>\n      <td>0.608</td>\n      <td>0.345</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0886</td>\n      <td>0.1580</td>\n      <td>10.0</td>\n      <td>-15.528</td>\n      <td>0.0</td>\n      <td>0.0368</td>\n      <td>0.720000</td>\n      <td>0.964000</td>\n      <td>0.115</td>\n      <td>0.0376</td>\n      <td>86.050</td>\n      <td>117840.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>8500 rows  20 columns</p>\n</div>"
     },
     "execution_count": 605,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"score_prediction\", \"Popularity\"]]\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}